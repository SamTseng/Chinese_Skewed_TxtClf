{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Comprehensive Guide to Understand and Implement Text Classification in Python\n",
    "\n",
    "The content of this file is originally from the article by Shivam Bansal, published on April 23, 2018 at: https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/. The code examples in the article have some errors (see the comments at the bottom of the article). But the article is still a good source to learn various ML and DL methods in Python for text classification.\n",
    "\n",
    "The original content and code are modified by Yuen-Hsien Tseng during 2018/09/29~2020/02/10, and is used in the paper: \n",
    "\n",
    "Yuen-Hsien Tseng, \"[The Feasibility of Automated Topic Analysis: An Empirical Evaluation of Deep Learning Techniques Applied to Skew-Distributed Chinese Text Classification](http://joemls.dils.tku.edu.tw/fulltext/57104fullText.pdf),\" Journal of Educational Media & Library Sciences, Vol. 57, No. 1 (March 2020).\n",
    "\n",
    "### Note: \n",
    "\n",
    "1. To run deep learning methods, you need Word2Vec files in your local disk. I downloaded them from:\n",
    " https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh_classical.vec\n",
    " for Chinese Word2Vec, and from:\n",
    " https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.simple.vec\n",
    " for English Word2Vec.\n",
    " See the function Read_Word_Embedding() below to know where to save the Word2Vec files in your local disk as an example.\n",
    "\n",
    "2. The classifiers here are only for multi-class single-label text classification problems. For a text (document) belongs to multiple categories (multi-labeled), the methods shown here have to be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Text Classification is an example of supervised machine learning task since a labelled dataset containing text documents and their labels is used for train a classifier. An end-to-end text classification pipeline is composed of three main components:\n",
    "\n",
    "1. Dataset Preparation: The first step is the Dataset Preparation step which includes the process of loading a dataset and performing basic pre-processing. The dataset is then splitted into train and validation sets.\n",
    "\n",
    "2. Feature Engineering: The next step is the Feature Engineering in which the raw dataset is transformed into flat features which can be used in a machine learning model. This step also includes the process of creating new features from the existing data.\n",
    "\n",
    "3. Model Training: The final step is the Model Building step in which a machine learning model is trained on a labelled dataset.\n",
    "\n",
    "4. Improve Performance of Text Classifier: In this article, we will also look at the different ways to improve the performance of text classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting your machine ready\n",
    "Lets implement basic components in a step by step manner in order to create a text classification framework in python. To start with, import all the required libraries.\n",
    "\n",
    "You would need requisite libraries to run this code – you can install them at their individual official links:\n",
    "Pandas, \n",
    "Scikit-learn, \n",
    "XGBoost, \n",
    "TextBlob, \n",
    "Keras,\n",
    "jieba, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# libraries for dataset preparation, feature engineering, model training \n",
    "import time\n",
    "time_Start = time.time()\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset preparation\n",
    "To prepare the dataset, load the downloaded data into a pandas dataframe containing two columns – text and label. (Here are more text classification datasets: https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/kg/jcdj05xn20144cv9kwywp26r0000gn/T/jieba.cache\n",
      "Loading model cost 0.687 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 6.54 seconds to import packages.\n",
      "sys.argv: ['/Users/sam/anaconda3/envs/py3.6/lib/python3.6/site-packages/ipykernel_launcher.py', '-f', '/Users/sam/Library/Jupyter/runtime/kernel-e98056d7-3645-4bc7-8885-724c596b84f8.json']\n",
      "Enter the dataset file: Datasets/CnonC_All.txt\n",
      "Enter the number of training examples: 232\n",
      "Enter if word embedding layer is trainable (yes/no): yes\n",
      "min_df (terms whose tf lower than min_df is ignored): 1\n"
     ]
    }
   ],
   "source": [
    "import re, sys\n",
    "import jieba # to deal with Chinese text dataset\n",
    "jieba.load_userdict(\"TermFreq-utf8.txt\") # added on 2019/02/04\n",
    "\n",
    "print(\"It takes %4.2f seconds to import packages.\"%(time.time()-time_Start))\n",
    "\n",
    "print(\"sys.argv:\", sys.argv)\n",
    "Trainable = True # indicate if word embedding layer is trainable\n",
    "min_df=2\n",
    "if len(sys.argv) == 5 and re.match('\\d+', sys.argv[2]):\n",
    "    prog, data_file, TrainSize, Trainable, min_df = sys.argv\n",
    "elif len(sys.argv) == 4 and re.match('\\d+', sys.argv[2]): \n",
    "    prog, data_file, TrainSize, Trainable = sys.argv\n",
    "elif len(sys.argv) == 3 and re.match('\\d+', sys.argv[2]):\n",
    "    prog, data_file, TrainSize = sys.argv\n",
    "else:\n",
    "    data_file = input(\"Enter the dataset file: \")\n",
    "    TrainSize = input(\"Enter the number of training examples: \")\n",
    "    Trainable = input(\"Enter if word embedding layer is trainable (yes/no): \")\n",
    "    min_df = input(\"min_df (terms whose tf lower than min_df is ignored): \")\n",
    "\n",
    "min_df = int(min_df)\n",
    "TrainSize = int(TrainSize)\n",
    "Trainable = False if Trainable == 'no' else True\n",
    "\n",
    "#data_file = 'Datasets/PCWeb_All.txt'\n",
    "#TrainSize = 1190\n",
    "\n",
    "#data_file = 'Datasets/PCNews_All.txt'\n",
    "#TrainSize = 644\n",
    "\n",
    "#data_file = 'Datasets/joke_All.txt'\n",
    "#TrainSize = 2389\n",
    "\n",
    "#data_file = 'Datasets/CTC_All_sl.txt'\n",
    "#TrainSize = 19901\n",
    "\n",
    "#data_file = 'Datasets/Reuters_All_sl.txt'\n",
    "#TrainSize = 6561\n",
    "\n",
    "#data_file = '20news-bydate/20news-bydate_All.txt'\n",
    "#TrainSize = 11270\n",
    "\n",
    "#data_file = 'Datasets/CnonC_All.txt'\n",
    "#TrainSize = 232\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text): \n",
    "    '''\n",
    "    Given a raw text string, return a clean text string.\n",
    "    Example: \n",
    "        input:  \"Years  passed. 多少   年过 去 了 。  \"\n",
    "        output: \"years passed.多少年过去了。\"\n",
    "    '''\n",
    "# The next 4 lines are copied from: https://github.com/ahmedbesbes/overview-and-benchmark-of-traditional-and-deep-learning-models-in-text-classification\n",
    "    text = re.sub(r'http\\S+', '', text) # URL\n",
    "    text = re.sub(r\"#(\\w+)\", '', text) # hasttag in tweets\n",
    "    text = re.sub(r\"@(\\w+)\", '', text) # @domain\n",
    "    #text = re.sub(r'[^\\w\\s]', '', text) # remove non-word or non-space\n",
    "\n",
    "    text = text.lower() # 'years  passed. 多少   年过 去 了 。'\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "\n",
    "    # Next line will remove punctuations. \\w matches Chinese characters\n",
    "    #text = re.sub('\\W', ' ', text) # 'Years passed  多少 年过 去 了  '\n",
    "    # Next line will remove redundant white space for jeiba to cut\n",
    "    text = re.sub('\\s+([^a-zA-Z0-9.])', r'\\1', text) # years passed.多少年过去了。\n",
    "# see: https://stackoverflow.com/questions/16720541/python-string-replace-regular-expression\n",
    "    text = text.strip(' ')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This code block set 3 variables: PunctuationStr, Punctuations, StopWords\n",
    "\n",
    "# The English punctuations are from: https://keras.io/preprocessing/text/\n",
    "PunctuationStr = '''\n",
    "!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\n",
    "！＃＄％＆\\、（）＊＋，。/：；「」『』　■．．・…’’“”〝〞‵′。\n",
    "''' # there is a Chinese white space before '■'\n",
    "Punctuations = [x for x in PunctuationStr]\n",
    "\n",
    "# Combine Chinese and English stop words\n",
    "StopWords = '''\n",
    "的 是 了 和 與 及 或 於 也 並 之 以 在 另 又 該 由 但 仍 就\n",
    "都 讓 要 把 上 來 說 從 等 \n",
    "我 你 他 妳 她 它 您 我們 你們 妳們 他們 她們 \n",
    "並有 並可 可以 可供 提供 以及 包括 另有 另外 此外 除了 目前 現在 仍就 就是 \n",
    "'''.split()\n",
    "# StopWords.extend(['　', '■']) # these punctuations belong to r'\\W'\n",
    "# https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/stop_words.py\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "StopWords.extend(list(ENGLISH_STOP_WORDS))\n",
    "StopWords.extend('''said told '''.split())\n",
    "#print(StopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def count_words(words):\n",
    "    WL = [w for w in words if not re.match('\\s', w)]\n",
    "#    print(\"After remove white space, this list is:\", WL)\n",
    "    return len(WL)\n",
    "\n",
    "def clean_words(words):\n",
    "#    print(\"After jieba.lcut():\", words)\n",
    "#    WL = [ w \n",
    "#    WL = [ ps.stem(w)\n",
    "    WL = [ wnl.lemmatize(w)\n",
    "            for w in words if (not re.match('\\s', w)) and \n",
    "                (w not in StopWords) and\n",
    "                (w not in Punctuations) and \n",
    "                (not re.match('^\\W$', w)) and # can replace above line to remove single non-word\n",
    "                (not re.match('^\\d*\\.?\\d*%?$', w)) and # skip if decimal numbers or percentage\n",
    "                (not re.match('^\\.+$', w)) and # skip if '.', added on 2019/02/04\n",
    "                (not re.match('^[a-z_]$', w)) # skip if single lower case term\n",
    "         ]\n",
    "    return WL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCharsLen = 53 \n",
      "TextWordsLen = 30\n",
      "Avg Text Chars =  19\n",
      "Avg Text Words =   8\n",
      "It takes 1.28 seconds to load, segment, and clean data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CharList, WordList = [], []\n",
    "\n",
    "def Load_Data(file): # load the dataset\n",
    "    labels, texts = [], []\n",
    "    i = 0\n",
    "    for line in open(file, encoding='UTF-8').read().split(\"\\n\"):\n",
    "        if line == '': continue\n",
    "        (label, text) = line.split(\"\\t\") # load my own data\n",
    "        labels.append(label) # assume single label classification\n",
    "        CharList.append(len(text))\n",
    "#        print(\"Before text=\", text)\n",
    "#        texts.append(text); continue # 2019/02/03 for joke corpus, used only once for saving train and test files\n",
    "        words_list = jieba.lcut(clean_text(text)) # https://github.com/fxsjy/jieba\n",
    "        words = clean_words(words_list)\n",
    "        WordList.append(count_words(words_list))\n",
    "        texts.append(\" \".join(words)) # should be a string of words\n",
    "#        print(\"After clean_words(), texts[{}]='{}'\".format(i, texts[i]))\n",
    "#        print(\"word:\", words)\n",
    "#        i += 1\n",
    "#        if i>=5: break\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "    DF = pandas.DataFrame()\n",
    "    DF['text'] = texts\n",
    "    DF['label'] = labels\n",
    "    return DF\n",
    "\n",
    "time_LoadData = time.time()\n",
    "\n",
    "All_DF = Load_Data(data_file)\n",
    "\n",
    "#CharList = [len(c) for c in All_DF['text']]\n",
    "#WordList = [len(x.split()) for x in All_DF['text']]\n",
    "TextCharsLen = max(CharList)\n",
    "TextWordsLen = max(WordList)\n",
    "TextCharsAvg = sum(CharList) / len(CharList)\n",
    "TextWordsAvg = sum(WordList) / len(WordList)\n",
    "\n",
    "print(\"TextCharsLen =\", TextCharsLen, \"\\nTextWordsLen =\", TextWordsLen)\n",
    "print(\"Avg Text Chars = %3d\"%TextCharsAvg)\n",
    "print(\"Avg Text Words = %3d\"%TextWordsAvg)\n",
    "\n",
    "print(\"It takes %4.2f seconds to load, segment, and clean data.\"%(time.time()-time_LoadData))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. (https://en.wikipedia.org/wiki/Exploratory_data_analysis)\n",
    "\n",
    "EDA is a very important step which takes place after feature engineering and acquiring data and it should be done before any modeling. This is because it is very important for a data scientist to be able to understand the nature of the data without making assumptions. (https://datascienceguide.github.io/exploratory-data-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002-非營建類    166\n",
      "001-營建類     166\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cat2num = pandas.Series(All_DF['label']).value_counts()\n",
    "print(cat2num.sort_values(ascending=False))\n",
    "\n",
    "# The next function does the similar report, but assume multiple columns\n",
    "#   and each column (category) has 0 or 1 values\n",
    "# See: https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5\n",
    "def show_stats(All_DF):\n",
    "    df_label = All_DF.drop(['text'], axis=1)\n",
    "    counts = []\n",
    "    categories = list(df_label.columns.values)\n",
    "    print(categories)\n",
    "    for c in categories:\n",
    "        counts.append((c, df_label[c].sum()))\n",
    "    df_stats = pandas.DataFrame(counts, columns=['category', 'number_of_texts'])\n",
    "    print(df_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will split the dataset into training and validation sets so that we can train and test classifier.\n",
    "\n",
    "Also, we will encode our target column so that it can be used in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "# For shuffle=False to be effective, need scikit-learn 0.19\n",
    "# pip install --upgrade scikit-learn # but in my laptop, this does not work\n",
    "# Next line do not work because my scikit-learn is version 0.18.1\n",
    "#trainText_x, testText_x, train_yL, test_yL = model_selection.train_test_split(\n",
    "#    All_DF['text'], All_DF['label'], train_size=TrainSize, shuffle = False, stratify = None)\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/43838052/how-to-get-a-non-shuffled-train-test-split-in-sklearn\n",
    "def non_shuffling_train_test_split(X, y, train_size=0.7, test_size=0.3):\n",
    "    i = int((1 - test_size) * X.shape[0]) + 1\n",
    "    i = train_size # an integer to overwrite the above line\n",
    "    X_train, X_test = numpy.split(X, [i])\n",
    "    y_train, y_test = numpy.split(y, [i])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "trainText_x, testText_x, train_yL, test_yL = non_shuffling_train_test_split(\n",
    "    All_DF['text'], All_DF['label'], train_size=TrainSize)\n",
    "\n",
    "# label encode the target variable \n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "LabEncoder = preprocessing.LabelEncoder() # convert label name to label int\n",
    "train_y = LabEncoder.fit_transform(train_yL)\n",
    "test_y = LabEncoder.fit_transform(test_yL)\n",
    "Num_Classes = len(LabEncoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On 2019/02/03: This function is used only once to obtain the train and test files. \n",
    "def Save_Split_Train_Test(All_DF):\n",
    "# https://stackoverflow.com/questions/34842405/parameter-stratify-from-method-train-test-split-scikit-learn\n",
    "    trainText_x, testText_x, train_yL, test_yL = model_selection.train_test_split(\n",
    "    All_DF['text'], All_DF['label'], test_size=0.3, stratify=All_DF['label'], random_state=42)\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# https://stackoverflow.com/questions/34318141/zip-pandas-dataframes-into-a-new-dataframe\n",
    "    #train_DF = pandas.concat([train_yL, trainText_x.str.split().str.join('')], axis=1)\n",
    "    #test_DF = pandas.concat([test_yL, testText_x.str.split().str.join('')], axis=1)\n",
    "    # Because of the original texts in Load_Data(), use next 2 lines rather than the above 2 lines\n",
    "    train_DF = pandas.concat([train_yL, trainText_x], axis=1)\n",
    "    test_DF = pandas.concat([test_yL, testText_x], axis=1)\n",
    "# https://stackoverflow.com/questions/16923281/pandas-writing-dataframe-to-csv-file\n",
    "    train_DF.to_csv('Datasets/CnonC_train.txt', sep='\\t', encoding='utf-8', index=False, header=False)\n",
    "    test_DF.to_csv('Datasets/Cnonc_test.txt', sep='\\t', encoding='utf-8', index=False, header=False)\n",
    "    return trainText_x, testText_x, train_yL, test_yL\n",
    "\n",
    "# No need to call this function, if you have the train and test files already\n",
    "# trainText_x, testText_x, train_yL, test_yL = Save_Split_Train_Test(All_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data:  <class 'pandas.core.series.Series'> , shape: (332,) \n",
      " 0     001-營建類\n",
      "1    002-非營建類\n",
      "2    002-非營建類\n",
      "Name: label, dtype: object\n",
      "train_yL:  <class 'pandas.core.series.Series'> , shape: (232,) , unique: 2 \n",
      " 001-營建類     116\n",
      "002-非營建類    116\n",
      "Name: label, dtype: int64\n",
      "test_yL:   <class 'pandas.core.series.Series'> , shape: (100,) , unique: 2 \n",
      " 001-營建類     50\n",
      "002-非營建類    50\n",
      "Name: label, dtype: int64\n",
      "Num of Classes (Categories or Labels): 2\n",
      "Label Names [:5]: ['001-營建類' '002-非營建類']\n",
      "Label Names transformed[:5]: [0 1]\n",
      "Label inverse transform [0, 1]: ['001-營建類' '002-非營建類']\n"
     ]
    }
   ],
   "source": [
    "print(\"All data: \", type(All_DF['label']), \", shape:\", All_DF['label'].shape, \"\\n\", All_DF['label'][:3])\n",
    "#print(\"train_yL: \", type(train_yL), \", shape:\", train_yL.shape, \"\\n\", train_yL[:3])\n",
    "#print(\"train_y:  \", type(train_y),  \", shape:\", train_y.shape, \"\\n\", \"Label ID:\", train_y[:5])\n",
    "# See: https://stackoverflow.com/questions/38309729/count-unique-values-with-pandas-per-groups/38309823\n",
    "print(\"train_yL: \", type(train_yL), \", shape:\", train_yL.shape, \", unique:\", train_yL.nunique(), \"\\n\", train_yL.value_counts())\n",
    "print(\"test_yL:  \", type(test_yL),  \", shape:\", test_y.shape, \", unique:\", test_yL.nunique(), \"\\n\", test_yL.value_counts())\n",
    "\n",
    "print(\"Num of Classes (Categories or Labels):\", Num_Classes)\n",
    "print(\"Label Names [:5]:\", LabEncoder.classes_[:5]) # print label names\n",
    "print(\"Label Names transformed[:5]:\", LabEncoder.transform(LabEncoder.classes_[:5]))\n",
    "print(\"Label inverse transform [0, 1]:\", LabEncoder.inverse_transform([0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.preprocessing.label.LabelEncoder'> LabelEncoder()\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "train=232, test=100, sum=332, ration=30%\n",
      "[0 1 1 1 0 1 0 0 0 0] [1 1 1 1 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(type(LabEncoder), LabEncoder)\n",
    "print(type(train_y), type(test_y))\n",
    "m, n = len(train_y), len(test_y)\n",
    "print(\"train={}, test={}, sum={}, ration={}%\".format(m, n, m+n, int(0.5+(n/(m+n)*100))))\n",
    "print(train_y[:10], test_y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "The next step is the feature engineering step. In this step, raw text data will be transformed into feature vectors and new features will be created using the existing dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Count Vectors as features\n",
    "Count Vector is a document-to-term matrix notation of the dataset (corpus) in which:\n",
    "1. every row represents a document from the corpus, \n",
    "2. every column represents a term from the corpus, and \n",
    "3. every cell represents the frequency count of a particular term in a particular document.\n",
    "\n",
    "A similar term-to-document matrix is illustrated in the figure below:\n",
    "![](https://ahmedbesbes.com/images/article_5/tfidf.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 0.02 seconds to convert count vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "time_CountVector = time.time()\n",
    "\n",
    "def Create_CountVector():\n",
    "\n",
    "# Create a count vectorizer object.\n",
    "# It takes the steps of prepocessing, tokenizer, stopwording, ...\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "        stop_words=StopWords, max_df=0.98, min_df=min_df)\n",
    "    count_vect.fit(All_DF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "    xtrain_count = count_vect.transform(trainText_x)\n",
    "    xtest_count = count_vect.transform(testText_x)\n",
    "\n",
    "    print(\"It takes %4.2f seconds to convert count vectors.\"%(time.time()-time_CountVector))\n",
    "\n",
    "    return(xtrain_count, xtest_count, count_vect)\n",
    "\n",
    "(xtrain_count, xtest_count, count_vect) = Create_CountVector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'> CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.98, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['的', '是', '了', '和', '與', '及', '或', '於', '也', '並', '之', '以', '在', '另', '又', '該', '由', '但', '仍', '就', '都', '讓', '要', '把', '上', '來', '說', '從', '等', '我', '你', '他', '妳', '她', '它', '您', '我們', '你們', '妳們', '他們', '她們', '並有', '並可', '可以', '可供', '提供', '以及', '包括', '另有', '另外', '此外', '除了', '目前', '現在', ...ds', 'anyone', 'at', 'give', 'of', 'only', 'hers', 'from', 'take', 'had', 'perhaps', 'said', 'told'],\n",
      "        strip_accents=None, token_pattern='\\\\w{1,}', tokenizer=None,\n",
      "        vocabulary=None)\n",
      "<class 'scipy.sparse.csr.csr_matrix'> <class 'scipy.sparse.csr.csr_matrix'>\n",
      "xtrain_count.shape: (232, 1304)\n",
      "xtest_count.shape : (100, 1304)\n",
      "\n",
      "Used stop words:  frozenset({'her', 'empty', 'in', '以及', 'become', 'formerly', '除了', 'elsewhere', 'along', '及', 'both', 'full', 'a', 'could', 'everyone', 'twenty', '仍', 'bill', 'now', 'against', 'find', 'am', 'about', 'couldnt', 'during', 'they', '它', 'wherein', '把', 'system', 'whatever', '但', '了', 'least', 'whither', 'again', '來', 'always', 'where', 'third', 'cry', 'less', 'us', 'thereafter', 'found', 'meanwhile', '在', 'whose', 'myself', 'have', 'eight', 'own', 'mill', '並', '從', 'being', 'seeming', '妳們', 'amount', 'though', 'otherwise', 'below', 'five', 'whereas', 'around', '都', 'get', 'noone', 'itself', 'moreover', 'neither', 'out', 'anywhere', 'another', 'ours', 'whereby', '另外', 'herein', 'part', 'their', 'already', '就', 'been', '上', '目前', 'all', 'per', 'before', 'go', 'inc', 'himself', 'should', 'anyone', 'whom', 'up', 'whether', 'who', 'be', 'back', 'whole', 'how', 'top', '就是', 'yet', 'please', 'either', 'until', 'when', 'interest', 'might', 'fill', 'said', 'if', 'enough', 'yourselves', 'etc', 'somehow', 'made', 'why', '於', '仍就', 'four', '說', 'seems', 'ltd', 'fifty', 'it', '提供', 'nowhere', 'will', 'an', 'former', 'two', 'anyway', 'ever', 'six', '包括', 'latterly', '可以', 'over', 'with', 'except', 'many', 'more', 'others', '妳', 'ten', 'latter', 'indeed', '我們', 'then', 'them', 'thereby', 'nobody', 'whoever', 'to', 'yourself', 'each', 'into', 'our', '另有', 'thick', 'almost', 'keep', 'move', 'together', 'after', 'see', 'hundred', 'well', '或', 'on', 'hereby', 'also', 'without', 'fifteen', '你們', 'these', 'among', 'the', 'hasnt', 'sometime', '也', 'co', 'thin', 'at', 'of', 'give', 'hers', 'several', 'under', 'whenever', 'must', 're', '並有', '是', '等', 'him', 'my', 'because', 'me', 'sometimes', 'i', 'nothing', 'serious', 'alone', 'much', 'bottom', 'besides', 'any', '她', 'rather', 'afterwards', 'behind', 'were', '你', '該', '之', 'his', 'first', 'once', 'further', 'everywhere', 'there', 'do', 'your', 'put', 'never', 'amongst', 'other', 'de', 'front', '讓', 'very', 'often', 'last', 'thence', 'un', 'may', 'toward', 'above', 'onto', 'whereupon', 'next', 'twelve', 'con', 'beyond', 'since', 'themselves', 'becomes', 'three', 'therein', 'something', 'those', 'through', 'cant', '可供', 'cannot', '由', 'name', 'thus', 'hence', 'or', '的', 'due', 'ie', 'sincere', '並可', '與', 'for', 'this', 'anyhow', 'forty', 'namely', 'he', 'but', 'seem', 'most', 'so', 'same', 'seemed', 'which', 'would', 'mostly', 'mine', 'one', 'therefore', 'towards', 'only', 'had', 'perhaps', 'describe', '您', 'ourselves', 'throughout', 'within', 'eg', '現在', '和', 'hereafter', 'amoungst', 'while', 'we', 'still', 'call', 'few', '要', 'here', 'however', '我', 'side', 'show', 'was', 'hereupon', 'nor', 'yours', 'someone', 'can', 'too', 'although', '他', 'every', '她們', 'whereafter', 'became', 'somewhere', 'some', 'sixty', 'as', 'has', 'becoming', 'and', 'nine', 'eleven', 'fire', '他們', 'beside', 'are', 'thru', 'off', 'by', 'she', '此外', 'its', 'down', 'thereupon', 'even', 'between', '以', 'no', '又', 'you', 'beforehand', 'such', 'wherever', 'via', 'across', 'everything', 'what', 'that', 'else', 'anything', 'than', '另', 'is', 'nevertheless', 'done', 'upon', 'whence', 'herself', 'not', 'told', 'none', 'detail', 'from', 'take'})\n"
     ]
    }
   ],
   "source": [
    "def Print_count_vect():\n",
    "    print(type(count_vect), count_vect)\n",
    "    print(type(xtrain_count), type(xtest_count))\n",
    "    print(\"xtrain_count.shape:\", xtrain_count.shape)\n",
    "    print(\"xtest_count.shape :\", xtest_count.shape)\n",
    "# https://stackoverflow.com/questions/36967666/transform-scipy-sparse-csr-to-pandas\n",
    "# from scipy.sparse.csr import csr_matrix\n",
    "    # A = csr_matrix([[1, 0, 2], [0, 3, 0]]); print(A)\n",
    "    # df = pd.DataFrame(A.toarray()); print(df)\n",
    "    #print(xtrain_count)\n",
    "    #print(xtest_count[0, 0:10])\n",
    "    print(\"\\nUsed stop words: \", count_vect.get_stop_words())\n",
    "    \n",
    "Print_count_vect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TF-IDF Vectors as features\n",
    "TF-IDF score represents the relative importance of a term in the document and the entire corpus. TF-IDF score is composed by two terms: the first computes the normalized Term Frequency (TF), the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams)\n",
    "\n",
    "1. Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents\n",
    "2. N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams\n",
    "3. Character Level TF-IDF : Matrix representing tf-idf scores of character level n-grams in the corpus\n",
    "\n",
    "A word level n-gram can be illustrated in the figure below (from https://ahmedbesbes.com/images/article_5/ngrams.png):\n",
    "![](https://ahmedbesbes.com/images/article_5/ngrams.png)\n",
    "\n",
    "A character level n-gram can be illustrated in the figure below )from https://ahmedbesbes.com/images/article_5/ngrams_char.jpg):\n",
    "![](https://ahmedbesbes.com/images/article_5/ngrams_char.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain_tfidf.shape: (232, 1304)\n",
      "xtest_tfidf.shape : (100, 1304)\n",
      "xtrain_tfidf_ngram.shape: (232, 3482)\n",
      "xtest_tfidf_ngram.shape : (100, 3482)\n",
      "xtrain_tfidf_ngram_chars.shape: (232, 6583)\n",
      "xtest_tfidf_ngram_chars.shape : (100, 6583)\n",
      "It takes 0.08 seconds to convert 3 TFxIDF vectors.\n"
     ]
    }
   ],
   "source": [
    "time_TfidfVector = time.time()\n",
    "\n",
    "def Create_TFxIDF():\n",
    "\n",
    "# word level tf-idf\n",
    "    #tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000)\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "        stop_words=StopWords, max_df=0.98, min_df=min_df, max_features=10000)\n",
    "    tfidf_vect.fit(All_DF['text'])\n",
    "    xtrain_tfidf = tfidf_vect.transform(trainText_x)\n",
    "    xtest_tfidf = tfidf_vect.transform(testText_x)\n",
    "    print(\"xtrain_tfidf.shape:\", xtrain_tfidf.shape)\n",
    "    print(\"xtest_tfidf.shape :\", xtest_tfidf.shape)\n",
    "\n",
    "# word level ngram tf-idf \n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                    stop_words=StopWords, max_df=0.98, min_df=min_df,\n",
    "                    nmgra_range=(2,3), max_features=10000)\n",
    "    tfidf_vect_ngram.fit(All_DF['text'])\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(trainText_x)\n",
    "    xtest_tfidf_ngram =  tfidf_vect_ngram.transform(testText_x)\n",
    "    print(\"xtrain_tfidf_ngram.shape:\", xtrain_tfidf_ngram.shape)\n",
    "    print(\"xtest_tfidf_ngram.shape :\", xtest_tfidf_ngram.shape)\n",
    "\n",
    "\n",
    "# character level ngram tf-idf\n",
    "    tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', \n",
    "                    stop_words=StopWords, max_df=0.98, min_df=min_df,\n",
    "                    ngram_range=(2,3), max_features=10000)\n",
    "    tfidf_vect_ngram_chars.fit(All_DF['text'])\n",
    "    xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(trainText_x) \n",
    "    xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(testText_x) \n",
    "    print(\"xtrain_tfidf_ngram_chars.shape:\", xtrain_tfidf_ngram_chars.shape)\n",
    "    print(\"xtest_tfidf_ngram_chars.shape :\", xtest_tfidf_ngram_chars.shape)\n",
    "\n",
    "    print(\"It takes %4.2f seconds to convert 3 TFxIDF vectors.\"%(time.time()-time_TfidfVector))\n",
    "\n",
    "    return (xtrain_tfidf, xtest_tfidf, \n",
    "             xtrain_tfidf_ngram, xtest_tfidf_ngram,\n",
    "             xtrain_tfidf_ngram_chars, xtest_tfidf_ngram_chars,\n",
    "            tfidf_vect, tfidf_vect_ngram, tfidf_vect_ngram_chars)\n",
    "\n",
    "(xtrain_tfidf, xtest_tfidf, \n",
    " xtrain_tfidf_ngram, xtest_tfidf_ngram,\n",
    " xtrain_tfidf_ngram_chars, xtest_tfidf_ngram_chars,\n",
    " tfidf_vect, tfidf_vect_ngram, tfidf_vect_ngram_chars) = Create_TFxIDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Word Embeddings\n",
    "A word embedding is a form of representing words and documents using a dense vector representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. Word embeddings can be trained using the input corpus itself or can be generated using pre-trained word embeddings such as Glove, FastText, and Word2Vec. Any one of them can be downloaded and used as transfer learning. One can read more about word embeddings at: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/.\n",
    "\n",
    "Following snnipet shows how to use pre-trained word embeddings in the model. There are four essential steps:\n",
    "\n",
    "1. Loading the pretrained word embeddings\n",
    "2. Creating a tokenizer object\n",
    "3. Transforming text documents to sequence of tokens and pad them\n",
    "4. Create a mapping of token and their respective embeddings\n",
    "\n",
    "You can download the pre-trained word embeddings from: https://fasttext.cc/docs/en/pretrained-vectors.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totoal Chinese embedding words: 10696, embedding vector size: 300\n",
      "Warning: fail to read line at line 62776\n",
      "Warning: fail to read line at line 105579\n",
      "Warning: fail to read line at line 110602\n",
      "Totoal English embedding words: 111051, embedding vector size: 300\n",
      "It takes 6.84 seconds to load embedding vectors.\n"
     ]
    }
   ],
   "source": [
    "time_ReadWordEmbed = time.time()\n",
    "\n",
    "def Read_Word_Embedding():\n",
    "# load the pre-trained word-embedding vectors \n",
    "    embeddings_index = {}\n",
    "# https://stackoverflow.com/questions/47118678/difference-between-fasttext-vec-and-bin-file\n",
    "# For Chinese Word2Vec, it is now (2020/02/07) available \n",
    "#  at: https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh_classical.vec\n",
    "    for i, line in enumerate(open('/Users/sam/data_exp/Corpora/TextClassification/wiki.zh_classical/wiki.zh_classical.vec')):\n",
    "        if i==0: continue # The first line is: \"10696 300\"\n",
    "        values = line.split()\n",
    "        embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "    print(\"Totoal Chinese embedding words: {}, embedding vector size: {}\".format(i,len(values)-1))\n",
    "# Chinese Embedding words: 10696, embedding vector size: 300\n",
    "\n",
    "# For English Word2Vec, it is now (2020/02/07) available \n",
    "#  at: https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.simple.vec\n",
    "    for i, line in enumerate(open('/Users/sam/data_exp/Corpora/TextClassification/wiki.simple/wiki.simple.vec')):\n",
    "        if i==0: continue # The first line is: \"111051 300\"\n",
    "        values = line.split()\n",
    "        try:\n",
    "            embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "        # ValueError: could not convert string to float: 'united'\n",
    "        except ValueError:\n",
    "            print(\"Warning: fail to read line at line\", i)\n",
    "        # Warning: fail to read line at line 62776\n",
    "        # Warning: fail to read line at line 105579\n",
    "        # Warning: fail to read line at line 110602\n",
    "    embedding_vector_size = len(values)-1\n",
    "    print(\"Totoal English embedding words: {}, embedding vector size: {}\".format(i, embedding_vector_size))\n",
    "\n",
    "\n",
    "    print(\"It takes %4.2f seconds to load embedding vectors.\"%(time.time()-time_ReadWordEmbed))\n",
    "\n",
    "    return (embeddings_index, embedding_vector_size)\n",
    "\n",
    "(embeddings_index, embedding_vector_size) = Read_Word_Embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in TextWordsLen: 30\n",
      "Number of (word, id) pairs in word_index: 1304\n",
      "word_index.items()[:5]: [('工程', 1), ('改善', 2), ('設備', 3), ('八十七年度', 4), ('八十八年度', 5)]\n",
      "train_seq_x.shape: (232, 30)\n",
      "test_seq_x.shape:  (100, 30)\n",
      "train_seq_x:\n",
      " [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0 356 357  32 358 359 360]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0 163 164 165 166 167 168 361 362  46  33]]\n",
      "Number of terms not in the pretrained word embedding: 937\n",
      "Number of StopWords and Punctuations being removed: 0\n",
      "embedding_matrix.shape: (1305, 300)\n",
      "\n",
      "It takes 0.02 seconds to build Chinese and English embedding vectors.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "time_BuildWordEmbed = time.time()\n",
    "\n",
    "# Do not call Remove_word_index() to change word_index, \n",
    "#   because the index has been assigned when calling token.fit_on_texts()\n",
    "def Remove_word_index():\n",
    "    print(\"Before removing stop words, word_index length:\", len(word_index))\n",
    "#https://stackoverflow.com/questions/11277432/how-to-remove-a-key-from-a-python-dictionary\n",
    "    for w in StopWords: word_index.pop(w, None)\n",
    "# English puntuations can be found in print(string.punctuation)\n",
    "    for w in Punctuations: word_index.pop(w, None)\n",
    "    print(\"After  removing stop words, word_index length:\", len(word_index))\n",
    "\n",
    "def Create_Word_Embedding():\n",
    "# create a tokenizer \n",
    "# for details, see: https://keras.io/preprocessing/text/ and\n",
    "#   Tokenizer API at: https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n",
    "# And: https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/\n",
    "    token = text.Tokenizer(filters=PunctuationStr)\n",
    "    token.fit_on_texts(All_DF['text']) # split on ' ', lowercasing English\n",
    "# fit_on_texts() does not provide stop words removal, what a pity!\n",
    "# fit_on_texts() create 4 attributes, see https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/\n",
    "    word_index = token.word_index # a dictionary with ('word', integer_id) \n",
    "    word_index_len = len(word_index)\n",
    "    print(\"Number of words in TextWordsLen:\", TextWordsLen)\n",
    "    print(\"Number of (word, id) pairs in word_index:\", word_index_len)\n",
    "    print(\"word_index.items()[:5]:\", list(word_index.items())[:5]) \n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "    train_seq_x = sequence.pad_sequences(token.texts_to_sequences(trainText_x), maxlen=TextWordsLen)\n",
    "    test_seq_x = sequence.pad_sequences(token.texts_to_sequences(testText_x), maxlen=TextWordsLen)\n",
    "\n",
    "    print(\"train_seq_x.shape:\", train_seq_x.shape)\n",
    "    print(\"test_seq_x.shape: \", test_seq_x.shape)\n",
    "    print(\"train_seq_x:\\n\", train_seq_x[:2])\n",
    "\n",
    "# create token-embedding mapping\n",
    "    OOV, s = 0, 0\n",
    "    embedding_matrix = numpy.zeros( (word_index_len + 1, embedding_vector_size) )\n",
    "    for word, i in word_index.items():\n",
    "    # Let the deep NN to learn features. \n",
    "    # So comment out the next 2 lines on 2018/09/07\n",
    "    #if word in StopWords: s+=1; continue # added by Sam Tseng on 2018/09/05\n",
    "    #if word in Punctuations: s+=1; continue # added by Sam Tseng on 2018/09/05\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "          OOV += 1\n",
    "\n",
    "# Number of (key, value) pairs in PCWeb dataset's word_index: 7028\n",
    "# Number of terms not in the Chinese pretrained word embedding: 5502\n",
    "    print(\"Number of terms not in the pretrained word embedding:\", OOV)\n",
    "    print(\"Number of StopWords and Punctuations being removed:\", s)\n",
    "    print(\"embedding_matrix.shape:\", embedding_matrix.shape)\n",
    "\n",
    "    print(\"\\nIt takes %4.2f seconds to build Chinese and English embedding vectors.\"%(time.time()-time_BuildWordEmbed))\n",
    "\n",
    "    return (train_seq_x, test_seq_x, word_index_len, embedding_matrix) \n",
    "\n",
    "(train_seq_x, test_seq_x, word_index_len, embedding_matrix) = Create_Word_Embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Text / NLP based features\n",
    "A number of extra text based features can also be created which sometimes are helpful for improving text classification models. Some examples are:\n",
    "\n",
    "1. Word Count of the documents – total number of words in the documents\n",
    "2. Character Count of the documents – total number of characters in the documents\n",
    "3. Average Word Density of the documents – average length of the words used in the documents\n",
    "4. Puncutation Count in the Complete Essay – total number of punctuation marks in the documents\n",
    "5. Upper Case Count in the Complete Essay – total number of upper count words in the documents\n",
    "6. Title Word Count in the Complete Essay – total number of proper case (title) words in the documents\n",
    "7. Frequency distribution of Part of Speech Tags:\n",
    "  1. Noun Count\n",
    "  2. Verb Count\n",
    "  3. Adjective Count\n",
    "  4. Adverb Count\n",
    "  5. Pronoun Count\n",
    "\n",
    "These features are highly experimental ones and should be used according to the problem statement only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "time_NLPstats = time.time()\n",
    "\n",
    "All_DF['char_count'] = All_DF['text'].apply(len)\n",
    "\n",
    "All_DF['word_count'] = All_DF['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "All_DF['word_density'] = All_DF['char_count'] / (All_DF['word_count']+1)\n",
    "\n",
    "All_DF['punctuation_count'] = All_DF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "\n",
    "All_DF['title_word_count'] = All_DF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "\n",
    "All_DF['upper_case_word_count'] = All_DF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "\n",
    "print(\"\\nIt takes %4.2f seconds to build NLP features.\"%(time.time()-time_NLPstats))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#print(\"char_count:\", All_DF['char_count'][:2])\n",
    "#print(All_DF.iloc[0:2, 1:])\n",
    "print(All_DF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "time_NLPfeatures = time.time()\n",
    "\n",
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "#Function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "All_DF['noun_count'] = All_DF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "All_DF['verb_count'] = All_DF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "All_DF['adj_count'] = All_DF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "All_DF['adv_count'] = All_DF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "All_DF['pron_count'] = All_DF['text'].apply(lambda x: check_pos_tag(x, 'pron'))\n",
    "\n",
    "print(\"\\nIt takes %4.2f seconds to build NLP features.\"%(time.time()-time_NLPfeature))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(All_DF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Topic Models as features\n",
    "Topic Modelling is a technique to identify the groups of words (called a topic) from a collection of documents that contains best information in the collection. \n",
    "\n",
    "Here Latent Dirichlet Allocation (LDA) is used for generating Topic Modelling Features. LDA is an iterative model which starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. Although the tokens themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents. One can read more about topic modelling at: https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['工程 改善 農路 設備 路面 城市 土 道路 拓寬 路', '八十七年度 第二次 工程 採購 用 公告 遴選 雲林縣 宣導 防制', '工程 公告 設備 八十七年度 線 中 69kv 購置 暨 軟體', '工程 整修 設備 八十七年度 技 裝 製 硝酸 儲槽 貨車', '型 件 西裝 長褲 上衣 內視鏡 中 採購案 工程 八十七年度', '工程 計畫 公告 地 用 技術 開發 公司 第二 段', '工程 更新 漁港 里 國小 興建 新建 小型 第四次 吋', '工程 改善 土 城市 登山 步道 八十八年度 附屬 清潔 電話', '維護 工程 路 345kv 電線 輸 明潭 二廠 大觀 鳳林', '保險 研究船 八十八年度 工程 硬質 一號 貴儀 聚乙烯 現期 一九九九年']\n",
      "\n",
      "It takes 0.40 seconds to train a topic model\n"
     ]
    }
   ],
   "source": [
    "time_TopicModel = time.time()\n",
    "\n",
    "def Create_Topic_Model():\n",
    "    # use global var: count_vect, xtrain_count\n",
    "# train a LDA Model\n",
    "# The next line cause error: \n",
    "    #lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "    lda_model = decomposition.LatentDirichletAllocation(learning_method='online', max_iter=20)\n",
    "\n",
    "    X_topics = lda_model.fit_transform(xtrain_count)\n",
    "    topic_word = lda_model.components_ \n",
    "    vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "    n_top_words = 10\n",
    "    topic_summaries = []\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "        topic_summaries.append(' '.join(topic_words))\n",
    "    print(topic_summaries[:20])\n",
    "\n",
    "    print(\"\\nIt takes %4.2f seconds to train a topic model\"%(time.time()-time_TopicModel))\n",
    "\n",
    "Create_Topic_Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Building\n",
    "The final step in the text classification framework is to train a classifier using the features created in the previous step. There are many different choices of machine learning models which can be used to train a final model. We will implement following different classifiers for this purpose:\n",
    "\n",
    "1. Naive Bayes Classifier\n",
    "2. Linear Classifier\n",
    "3. Support Vector Machine\n",
    "4. Bagging Models\n",
    "5. Boosting Models\n",
    "6. Shallow Neural Networks\n",
    "7. Deep Neural Networks\n",
    "  1. Convolutional Neural Network (CNN)\n",
    "  2. Long Short Term Modelr (LSTM)\n",
    "  3. Gated Recurrent Unit (GRU)\n",
    "  4. Bidirectional RNN\n",
    "  5. Recurrent Convolutional Neural Network (RCNN)\n",
    "  6. Other Variants of Deep Neural Networks\n",
    "\n",
    "Lets implement these models and understand their details. The following function is a utility function which can be used to train a model. It accepts the classifier, feature_vector of training data, labels of training data and feature vectors of valid data as inputs. Using these inputs, the model is trained and accuracy score is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(classifier, feature_vector_train, label, feature_vector_test):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # predict the labels on test dataset\n",
    "    return classifier.predict(feature_vector_test), classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tcfunc(x, n=4): # trancate a number to have n decimal digits\n",
    "    d = '0' * n\n",
    "    d = int('1' + d)\n",
    "# https://stackoverflow.com/questions/4541155/check-if-a-number-is-int-or-float\n",
    "    if isinstance(x, (int, float)): return int(x * d) / d\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "#import itertools # replace this line by next line on 2019/01/03, because cannot find itertools for Python 3.6.7\n",
    "import more_itertools\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, numpy.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm) # print out consufion matrix\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = numpy.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "#    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "# Replace the above line by the next line on 2019/01/03, because cannot find itertools for Python 3.6.7\n",
    "    for i, j in more_itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use global variables:\n",
    "#  test_y\n",
    "#  LabEncoder.classes_\n",
    "def show_confusion_matrix(predictions):\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(test_y, predictions)\n",
    "    numpy.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=LabEncoder.classes_ ,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=LabEncoder.classes_ , normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# use a global variable: test_y\n",
    "def show_Result(predictions):\n",
    "    print(predictions[:10])\n",
    "\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "#    print(\"MicroF1 = %0.4f, MacroF1=%0.4f\" %\n",
    "#       (metrics.f1_score(test_y, predictions, average='micro'),\n",
    "#        metrics.f1_score(test_y, predictions, average='macro')))\n",
    "# https://stackoverflow.com/questions/455612/limiting-floats-to-two-decimal-points\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
    "    print(\"\\tPrecision\\tRecall\\tF1\\tSupport\")\n",
    "    (Precision, Recall, F1, Support) = list(map(tcfunc, \n",
    "        precision_recall_fscore_support(test_y, predictions, average='micro')))\n",
    "    print(\"Micro\\t{}\\t{}\\t{}\\t{}\".format(Precision, Recall, F1, Support))\n",
    "    (Precision, Recall, F1, Support) = list(map(tcfunc, \n",
    "        precision_recall_fscore_support(test_y, predictions, average='macro')))\n",
    "    print(\"Macro\\t{}\\t{}\\t{}\\t{}\".format(Precision, Recall, F1, Support))\n",
    "    \n",
    "#    if True:\n",
    "    if False:\n",
    "        print(confusion_matrix(test_y, predictions))\n",
    "        try: \n",
    "            print(classification_report(test_y, predictions, digits=4))\n",
    "        except ValueError:\n",
    "            print('May be some category has no predicted samples')\n",
    "        show_confusion_matrix(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Naive Bayes\n",
    "Implementing a naive bayes model using sklearn implementation with different features\n",
    "\n",
    "Naive Bayes is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature at: https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next function does not work\n",
    "def no_use_most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10):\n",
    "    labelid = list(classifier.classes_).index(classlabel)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn = sorted(zip(classifier.coef_[labelid], feature_names))[-n:]\n",
    "    for coef, feat in topn:\n",
    "        print(classlabel, feat, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is modified from: https://gist.github.com/bbengfort/044682e76def583a12e6c09209c664a1\n",
    "# and from: https://stackoverflow.com/questions/26976362/how-to-get-most-informative-features-for-scikit-learn-classifier-for-different-c\n",
    "# This function only works for binary classes\n",
    "def most_informative_feature_for_class(vectorizer, classifier, labels, n=10):\n",
    "    coefs = sorted( # Zip the feature names with the coefs and sort\n",
    "        zip(classifier.coef_[0], vectorizer.get_feature_names()))\n",
    "    topn  = zip(coefs[:n], coefs[:-(n+1):-1])\n",
    "    # Create two columns with most negative and most positive features.\n",
    "    for (cp, fnp), (cn, fnn) in topn:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (cp, fnp, cn, fnn))\n",
    "\n",
    "# nltk.classify.NaiveBayesClassifier has a show_most_informative_features()\n",
    "# You may compare the result here with those at: https://www.twilio.com/blog/2017/09/sentiment-analysis-python-messy-data-nltk.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NB, Count Vectors: \n",
      "[1 1 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.87\t0.87\t0.87\tNone\n",
      "Macro\t0.8713\t0.87\t0.8698\tNone\n",
      "\t-7.6760\t18k            \t\t-4.5405\t設備             \n",
      "\t-7.6760\t20k            \t\t-4.6315\t工程             \n",
      "\t-7.6760\t21k            \t\t-5.0370\t八十七年度          \n",
      "\t-7.6760\t22k            \t\t-5.1111\t公告             \n",
      "\t-7.6760\t264k           \t\t-5.3734\t電腦             \n",
      "\t-7.6760\t266k           \t\t-5.3734\t採購             \n",
      "\t-7.6760\t29k            \t\t-5.4788\t型              \n",
      "\t-7.6760\t3              \t\t-5.5966\t系統             \n",
      "\t-7.6760\t32k            \t\t-5.7301\t計畫             \n",
      "\t-7.6760\t345kv          \t\t-5.7301\t工作             \n",
      "\n",
      "NB, WordLevel TF-IDF: \n",
      "[1 1 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.87\t0.87\t0.87\tNone\n",
      "Macro\t0.8701\t0.87\t0.8699\tNone\n",
      "\t-7.3784\t18k            \t\t-5.5366\t設備             \n",
      "\t-7.3784\t20k            \t\t-5.9175\t八十七年度          \n",
      "\t-7.3784\t21k            \t\t-5.9228\t採購             \n",
      "\t-7.3784\t22k            \t\t-5.9875\t電腦             \n",
      "\t-7.3784\t264k           \t\t-6.0224\t公告             \n",
      "\t-7.3784\t266k           \t\t-6.1079\t型              \n",
      "\t-7.3784\t29k            \t\t-6.1819\t乙批             \n",
      "\t-7.3784\t3              \t\t-6.2511\t系統             \n",
      "\t-7.3784\t32k            \t\t-6.2695\t工程             \n",
      "\t-7.3784\t345kv          \t\t-6.3047\t工作             \n",
      "\n",
      "NB, N-Gram Vectors: \n",
      "[1 1 1 1 0 0 1 1 1 1]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.8\t0.8\t0.8\tNone\n",
      "Macro\t0.8255\t0.8\t0.796\tNone\n",
      "\t-8.2582\t18k 18k        \t\t-7.3217\t生啤酒 鋁罐         \n",
      "\t-8.2582\t18k 18k 左側     \t\t-7.4125\t設備 乙批          \n",
      "\t-8.2582\t18k 左側         \t\t-7.5651\t八十六年度 活性碳      \n",
      "\t-8.2582\t18k 左側 路基      \t\t-7.5651\t中文 圖書          \n",
      "\t-8.2582\t20k 21k        \t\t-7.5651\t丁酮 項           \n",
      "\t-8.2582\t20k 21k 東港     \t\t-7.5862\t週邊 設備          \n",
      "\t-8.2582\t21k 東港         \t\t-7.6042\t人孔 蓋           \n",
      "\t-8.2582\t21k 東港 三西      \t\t-7.6612\t電腦 設備          \n",
      "\t-8.2582\t22k 克勤橋        \t\t-7.6690\t五股 油庫          \n",
      "\t-8.2582\t22k 克勤橋 改建工程   \t\t-7.7063\t第二次 採購         \n",
      "NB, CharLevel Vectors: \n",
      "[1 1 1 1 0 0 0 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.84\t0.84\t0.8399\tNone\n",
      "Macro\t0.8489\t0.84\t0.8389\tNone\n",
      "\t-8.8916\t 21            \t\t-7.7006\t設備             \n",
      "\t-8.8916\t 22            \t\t-7.7006\t 設備            \n",
      "\t-8.8916\t 26            \t\t-7.7014\t八十             \n",
      "\t-8.8916\t 29            \t\t-7.7250\t 設             \n",
      "\t-8.8916\t 32            \t\t-7.7301\t度              \n",
      "\t-8.8916\t 34            \t\t-7.7498\t年度             \n",
      "\t-8.8916\t 36            \t\t-7.7498\t年度             \n",
      "\t-8.8916\t 38            \t\t-7.8118\t備              \n",
      "\t-8.8916\t 4             \t\t-7.8185\t型              \n",
      "\t-8.8916\t 40            \t\t-7.8468\t設備             \n",
      "\n",
      "It takes 0.04 seconds for Naive Bayes.\n"
     ]
    }
   ],
   "source": [
    "def Run_NaiveBayes():\n",
    "    \n",
    "    time_NaiveBayes = time.time()\n",
    "\n",
    "# Naive Bayes on Count Vectors   \n",
    "    predict, clf = train_predict(naive_bayes.MultinomialNB(), xtrain_count, train_y, xtest_count)\n",
    "    print(\"\\nNB, Count Vectors: \")\n",
    "    show_Result(predict)\n",
    "    most_informative_feature_for_class(count_vect, clf, train_yL, n=10)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "    predict, clf = train_predict(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "    print(\"\\nNB, WordLevel TF-IDF: \")\n",
    "    show_Result(predict)\n",
    "    #most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10)\n",
    "    most_informative_feature_for_class(tfidf_vect, clf, train_y, n=10)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "    predict, clf = train_predict(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "    print(\"\\nNB, N-Gram Vectors: \")\n",
    "    show_Result(predict)\n",
    "    most_informative_feature_for_class(tfidf_vect_ngram, clf, train_y, n=10)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "    predict, clf = train_predict(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars)\n",
    "    print(\"NB, CharLevel Vectors: \")\n",
    "    show_Result(predict)\n",
    "    most_informative_feature_for_class(tfidf_vect_ngram_chars, clf, train_y, n=10)\n",
    "\n",
    "    print(\"\\nIt takes %4.2f seconds for Naive Bayes.\"%(time.time()-time_NaiveBayes))\n",
    "\n",
    "Run_NaiveBayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Linear Classifier\n",
    "Implementing a Linear Classifier (Logistic Regression)\n",
    "\n",
    "Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic/sigmoid function. One can read more about logistic regression at: https://www.analyticsvidhya.com/blog/2015/10/basics-logistic-regression/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LR, Count Vectors: \n",
      "[1 1 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.91\t0.91\t0.91\tNone\n",
      "Macro\t0.9141\t0.91\t0.9097\tNone\n",
      "\t-2.8382\t工程             \t\t1.1360\t設備             \n",
      "\t-0.8465\t漁港             \t\t0.8256\t系統             \n",
      "\t-0.7653\t改善             \t\t0.7800\t公告             \n",
      "\t-0.7046\t中寮             \t\t0.7002\t第三次            \n",
      "\t-0.6529\t線              \t\t0.6797\t購置             \n",
      "\t-0.6470\t里              \t\t0.6727\t油庫             \n",
      "\t-0.5590\t中城蝕溝           \t\t0.6727\t五股             \n",
      "\t-0.5590\t控制工程           \t\t0.5623\t更新             \n",
      "\t-0.5351\t大武崙            \t\t0.5491\t型              \n",
      "\t-0.5301\t嘉民二路           \t\t0.5286\t儀控             \n",
      "\n",
      "LR, WordLevel TF-IDF: \n",
      "[1 1 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.89\t0.89\t0.89\tNone\n",
      "Macro\t0.8914\t0.89\t0.8899\tNone\n",
      "\t-3.1718\t工程             \t\t1.2851\t設備             \n",
      "\t-1.6856\t改善             \t\t0.8183\t採購             \n",
      "\t-0.7672\t農路             \t\t0.7652\t公告             \n",
      "\t-0.7254\t路面             \t\t0.7602\t型              \n",
      "\t-0.6905\t里              \t\t0.6811\t系統             \n",
      "\t-0.6899\t漁港             \t\t0.6681\t電腦             \n",
      "\t-0.6402\t新建             \t\t0.5795\t工作             \n",
      "\t-0.5866\t線              \t\t0.5411\t乙批             \n",
      "\t-0.5841\t道路             \t\t0.5203\t購置             \n",
      "\t-0.5319\t中寮             \t\t0.4670\t內視鏡            \n",
      "\n",
      "LR, N-Gram Vectors: \n",
      "[1 1 1 1 1 0 1 1 1 1]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.71\t0.71\t0.71\tNone\n",
      "Macro\t0.7964\t0.71\t0.6871\tNone\n",
      "\t-1.5406\t改善 工程          \t\t0.5471\t生啤酒 鋁罐         \n",
      "\t-0.5971\t農路 改善          \t\t0.4624\t設備 乙批          \n",
      "\t-0.5971\t農路 改善 工程       \t\t0.3895\t八十六年度 活性碳      \n",
      "\t-0.5633\t新建 工程          \t\t0.3895\t中文 圖書          \n",
      "\t-0.4955\t土 城市           \t\t0.3895\t丁酮 項           \n",
      "\t-0.4127\t中城蝕溝 控制工程      \t\t0.3471\t人孔 蓋           \n",
      "\t-0.4127\t中寮 嘉民二路        \t\t0.3353\t週邊 設備          \n",
      "\t-0.4073\t整修 工程          \t\t0.3200\t五股 油庫          \n",
      "\t-0.3621\t興建 工程          \t\t0.2996\t電腦 設備          \n",
      "\t-0.3607\t大武崙 漁港         \t\t0.2798\t第二次 採購         \n",
      "\n",
      "LR, CharLevel Vectors: \n",
      "[1 1 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.89\t0.89\t0.89\tNone\n",
      "Macro\t0.8914\t0.89\t0.8899\tNone\n",
      "\t-1.4799\t工程             \t\t0.5849\t設備             \n",
      "\t-1.4098\t 工程            \t\t0.5849\t 設備            \n",
      "\t-1.2249\t 工             \t\t0.5100\t型              \n",
      "\t-1.0285\t路              \t\t0.5070\t用              \n",
      "\t-0.7403\t 改善            \t\t0.4722\t備              \n",
      "\t-0.7403\t善              \t\t0.4526\t 設             \n",
      "\t-0.7403\t改善             \t\t0.4378\t設備             \n",
      "\t-0.7403\t改善             \t\t0.4190\t採購             \n",
      "\t-0.7259\t 改             \t\t0.4190\t 採購            \n",
      "\t-0.7186\t善 工            \t\t0.4049\t 採             \n",
      "\n",
      "It takes 0.05 seconds for Logistic Regression.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def Run_LogisticRegret():\n",
    "    \n",
    "    time_LogisticRegret = time.time()\n",
    "\n",
    "# Linear Classifier on Count Vectors\n",
    "    predict, clf = train_predict(linear_model.LogisticRegression(), xtrain_count, train_y, xtest_count)\n",
    "    print(\"\\nLR, Count Vectors: \")\n",
    "    show_Result(predict)\n",
    "    most_informative_feature_for_class(count_vect, clf, train_y, n=10)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "    predict, clf = train_predict(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "    print(\"\\nLR, WordLevel TF-IDF: \")\n",
    "    show_Result(predict)\n",
    "    most_informative_feature_for_class(tfidf_vect, clf, train_y, n=10)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "    predict, clf = train_predict(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "    print(\"\\nLR, N-Gram Vectors: \")\n",
    "    show_Result(predict)\n",
    "    most_informative_feature_for_class(tfidf_vect_ngram, clf, train_y, n=10)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "    predict, clf = train_predict(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars)\n",
    "    print(\"\\nLR, CharLevel Vectors: \")\n",
    "    show_Result(predict)\n",
    "    most_informative_feature_for_class(tfidf_vect_ngram_chars, clf, train_y, n=10)\n",
    "\n",
    "    print(\"\\nIt takes %4.2f seconds for Logistic Regression.\"%(time.time()-time_LogisticRegret))\n",
    "\n",
    "Run_LogisticRegret()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Implementing a SVM Model\n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. The model extracts a best possible hyper-plane / line that segregates the two classes. One can read more about it at: https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM, Count Vectors: \n",
      "[1 1 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.91\t0.91\t0.91\tNone\n",
      "Macro\t0.9141\t0.91\t0.9097\tNone\n",
      "\t-1.1119\t工程             \t\t0.4263\t第三次            \n",
      "\t-0.6617\t中寮             \t\t0.4025\t油庫             \n",
      "\t-0.6317\t中城蝕溝           \t\t0.4025\t五股             \n",
      "\t-0.6317\t控制工程           \t\t0.3940\t購置             \n",
      "\t-0.6117\t嘉民二路           \t\t0.3928\t系統             \n",
      "\t-0.5758\t漁港             \t\t0.3689\t活性碳            \n",
      "\t-0.4376\t線              \t\t0.3568\t儀控             \n",
      "\t-0.4133\t大武崙            \t\t0.3495\t設備             \n",
      "\t-0.3934\t整建工程           \t\t0.3439\t更新             \n",
      "\t-0.3444\t里              \t\t0.3086\t車三輛            \n",
      "\n",
      "SVM, WordLevel TF-IDF: \n",
      "[1 1 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.89\t0.89\t0.89\tNone\n",
      "Macro\t0.8914\t0.89\t0.8899\tNone\n",
      "\t-3.2724\t工程             \t\t1.0027\t設備             \n",
      "\t-1.2323\t改善             \t\t0.7850\t系統             \n",
      "\t-0.7990\t里              \t\t0.7375\t公告             \n",
      "\t-0.7938\t中寮             \t\t0.7019\t型              \n",
      "\t-0.7566\t漁港             \t\t0.6825\t購置             \n",
      "\t-0.7248\t線              \t\t0.6338\t油庫             \n",
      "\t-0.6364\t道路             \t\t0.6338\t五股             \n",
      "\t-0.6298\t大武崙            \t\t0.6292\t工作             \n",
      "\t-0.6295\t新建             \t\t0.6027\t採購             \n",
      "\t-0.6052\t設施             \t\t0.5927\t活性碳            \n",
      "\n",
      "SVM, N-Gram Vectors: \n",
      "[1 1 1 1 1 0 1 1 1 1]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.72\t0.72\t0.72\tNone\n",
      "Macro\t0.8015\t0.72\t0.6996\tNone\n",
      "\t-1.8645\t改善 工程          \t\t0.6898\t生啤酒 鋁罐         \n",
      "\t-0.8453\t新建 工程          \t\t0.6081\t八十六年度 活性碳      \n",
      "\t-0.7253\t中寮 嘉民二路        \t\t0.6081\t中文 圖書          \n",
      "\t-0.7253\t中城蝕溝 控制工程      \t\t0.6081\t丁酮 項           \n",
      "\t-0.6989\t土 城市           \t\t0.5661\t設備 乙批          \n",
      "\t-0.5835\t整修 工程          \t\t0.5101\t五股 油庫          \n",
      "\t-0.5818\t大武崙 漁港         \t\t0.4985\t人孔 蓋           \n",
      "\t-0.5398\t興建 工程          \t\t0.4122\t週邊 設備          \n",
      "\t-0.5198\t工程 第二次         \t\t0.4115\t第二次 採購         \n",
      "\t-0.5062\t登山 步道          \t\t0.4105\t電腦 設備          \n",
      "\n",
      "SVM, CharLevel Vectors: \n",
      "[1 1 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.91\t0.91\t0.91\tNone\n",
      "Macro\t0.9114\t0.91\t0.9099\tNone\n",
      "\t-1.6554\t工程             \t\t0.5354\t用              \n",
      "\t-1.5432\t 工程            \t\t0.5173\t設備             \n",
      "\t-1.3310\t 工             \t\t0.5173\t 設備            \n",
      "\t-0.9623\t路              \t\t0.4614\t告              \n",
      "\t-0.8083\t建              \t\t0.4592\t車              \n",
      "\t-0.6588\t道              \t\t0.4454\t型              \n",
      "\t-0.5769\t區              \t\t0.4296\t公告             \n",
      "\t-0.5645\t里              \t\t0.4292\t00             \n",
      "\t-0.5440\t山              \t\t0.3970\t 工作            \n",
      "\t-0.5428\t 改             \t\t0.3853\t公告             \n",
      "\n",
      "It takes 0.04 seconds for Linear SVM.\n"
     ]
    }
   ],
   "source": [
    "def Run_SVM():\n",
    "    \n",
    "    time_LinearSVM = time.time()\n",
    "\n",
    "# Use of class_weight='balanced' decrease accuracy, although PCWeb is unbalanced\n",
    "#accuracy = train_model(svm.SVC(class_weight='balanced'), xtrain_count, train_y, xtest_count)\n",
    "# LinearSVC() is much much better than SVC()\n",
    "    predict, clf = train_predict(svm.LinearSVC(), xtrain_count, train_y, xtest_count)\n",
    "    print(\"\\nSVM, Count Vectors: \")\n",
    "    show_Result(predict)\n",
    "    most_informative_feature_for_class(count_vect, clf, train_y, n=10)\n",
    "\n",
    "    predict, clf = train_predict(svm.LinearSVC(), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "    print(\"\\nSVM, WordLevel TF-IDF: \")\n",
    "    show_Result(predict)\n",
    "    most_informative_feature_for_class(tfidf_vect, clf, train_y, n=10)\n",
    "\n",
    "    predict, clf = train_predict(svm.LinearSVC(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "    print(\"\\nSVM, N-Gram Vectors: \")\n",
    "    show_Result(predict)\n",
    "    most_informative_feature_for_class(tfidf_vect_ngram, clf, train_y, n=10)\n",
    "\n",
    "    predict, clf = train_predict(svm.LinearSVC(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars)\n",
    "    print(\"\\nSVM, CharLevel Vectors: \")\n",
    "    show_Result(predict)\n",
    "    most_informative_feature_for_class(tfidf_vect_ngram_chars, clf, train_y, n=10)\n",
    "\n",
    "    print(\"\\nIt takes %4.2f seconds for Linear SVM.\"%(time.time()-time_LinearSVM))\n",
    "\n",
    "Run_SVM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Bagging Model\n",
    "Implementing a Random Forest Model\n",
    "\n",
    "Random Forest models are a type of ensemble models, particularly bagging models. They are part of the tree based model family. One can read more about Bagging and random forests at: https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RF, Count Vectors: \n",
      "[1 1 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.9\t0.9\t0.9\tNone\n",
      "Macro\t0.9058\t0.9\t0.8996\tNone\n",
      "\n",
      "RF, WordLevel TF-IDF: \n",
      "[1 1 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.9\t0.9\t0.9\tNone\n",
      "Macro\t0.9025\t0.9\t0.8998\tNone\n",
      "\n",
      "RF, N-Gram Vectors: \n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.65\t0.65\t0.65\tNone\n",
      "Macro\t0.7657\t0.65\t0.6072\tNone\n",
      "\n",
      "RF, CharLevel Vectors: \n",
      "[1 1 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.92\t0.92\t0.92\tNone\n",
      "Macro\t0.9261\t0.92\t0.9197\tNone\n",
      "\n",
      "It takes 0.08 seconds for Random Forest.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/sam/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/sam/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/sam/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def Run_RdnForest():\n",
    "    \n",
    "    time_RdnForest = time.time()\n",
    "\n",
    "# RF on Count Vectors\n",
    "    predict, clf = train_predict(ensemble.RandomForestClassifier(), xtrain_count, train_y, xtest_count)\n",
    "    print(\"\\nRF, Count Vectors: \")\n",
    "    show_Result(predict)\n",
    "    #most_informative_feature_for_class(count_vect, clf, train_y, n=10)\n",
    "    #'RandomForestClassifier' object has no attribute 'coef_'\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "    predict, clf = train_predict(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "    print(\"\\nRF, WordLevel TF-IDF: \")\n",
    "    show_Result(predict)\n",
    "\n",
    "    predict, clf = train_predict(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "    print(\"\\nRF, N-Gram Vectors: \")\n",
    "    show_Result(predict)\n",
    "\n",
    "    predict, clf = train_predict(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars)\n",
    "    print(\"\\nRF, CharLevel Vectors: \")\n",
    "    show_Result(predict)\n",
    "\n",
    "    print(\"\\nIt takes %4.2f seconds for Random Forest.\"%(time.time()-time_RdnForest))\n",
    "\n",
    "Run_RdnForest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Boosting Model\n",
    "Implementing Xtereme Gradient Boosting Model\n",
    "\n",
    "Boosting models are another type of ensemble models part of tree based models. Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). Read more about these models at: https://www.analyticsvidhya.com/blog/2016/01/xgboost-algorithm-easy-steps/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_XGboost():\n",
    "# The XGboost takes 2330 seconds for the 20NG datasets. \n",
    "# So we do not use it.\n",
    "\n",
    "    time_XGboost = time.time()\n",
    "\n",
    "# Extereme Gradient Boosting on Count Vectors\n",
    "    predict, clf = train_predict(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xtest_count.tocsc())\n",
    "    print(\"\\nXgb, Count Vectors: \")\n",
    "    show_Result(predict)\n",
    "    #most_informative_feature_for_class(count_vect, clf, train_y, n=10)\n",
    "    #'XGBClassifier' object has no attribute 'coef_'\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "    predict, clf = train_predict(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xtest_tfidf.tocsc())\n",
    "    print(\"\\nXgb, WordLevel TF-IDF: \")\n",
    "    show_Result(predict)\n",
    "\n",
    "    predict, clf = train_predict(xgboost.XGBClassifier(), xtrain_tfidf_ngram.tocsc(), train_y, xtest_tfidf_ngram.tocsc())\n",
    "    print(\"\\nXgb, N-Gram Vectors: \")\n",
    "    show_Result(predict)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "    predict, clf = train_predict(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xtest_tfidf_ngram_chars.tocsc())\n",
    "    print(\"\\nXgb, CharLevel Vectors: \")\n",
    "    show_Result(predict)\n",
    "\n",
    "    print(\"\\nIt takes %4.2f seconds for XGboost.\"%(time.time()-time_XGboost))\n",
    "\n",
    "#Run_XGboost() # may require 100 times of clock time more than SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Shallow Neural Networks\n",
    "A neural network is a mathematical model that is designed to behave similar to biological neurons and nervous system. These models are used to recognize complex patterns and relationships that exists within a labelled data. A shallow neural network contains mainly three types of layers – input layer, hidden layer, and output layer:\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/OH3gI-1.png)\n",
    "(The figire above is from: https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/OH3gI-1.png.)\n",
    "\n",
    "Read more about neural networks at: https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Convert label ID into one-hot-encoding for all training and testing data\n",
    "y_Train_OneHot = np_utils.to_categorical(train_y)\n",
    "y_Test_OneHot = np_utils.to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "def plotHistory(train_history):\n",
    "    print(\"train history keys:\", train_history.history.keys())\n",
    "    # train history keys: dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(train_history.history['acc'])\n",
    "    if 'val_acc' in train_history.history.keys():\n",
    "        plt.plot(train_history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    if 'val_acc' in train_history.history.keys():\n",
    "        plt.legend(['train', 'validation'], loc='lower right')\n",
    "    else:\n",
    "        plt.legend(['train'], loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(train_history.history['loss'])\n",
    "    if 'val_loss' in train_history.history.keys():\n",
    "        plt.plot(train_history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    if 'val_loss' in train_history.history.keys():\n",
    "        plt.legend(['train', 'validation'], loc='upper right')\n",
    "    else:\n",
    "        plt.legend(['train'], loc='upper right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN_predict(classifier, feature_vector_train, label, feature_vector_valid, \n",
    "            batch_size=32,\n",
    "            epochs=10,\n",
    "            validation_split=0.0): # or validation_split=0.1): \n",
    "    \n",
    "    # fit the training dataset on the classifier\n",
    "    history=classifier.fit(feature_vector_train, label, verbose=1,\n",
    "            epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    # plotHistory(history)\n",
    "    \n",
    "    # https://stackoverflow.com/questions/38971293/get-class-labels-from-keras-functional-model\n",
    "    # Note: predict_classes() exists in Sequential, not in Functional model \n",
    "    # predict the labels on validation dataset\n",
    "    y_prob = classifier.predict(feature_vector_valid)\n",
    "    #print(\"y_prob:\\n\", y_prob[:3])\n",
    "    predictions = y_prob.argmax(axis=-1)\n",
    "    # or equivalently, use next line:\n",
    "    # predictions = y_classes = keras.np_utils.probas_to_classes(y_prob)\n",
    "    print(\"predictions:\\n\", predictions[:3])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_SimpleNN(input_size, output_size):\n",
    "    # create input layer \n",
    "    print(\"input_size:\", input_size, \", output_size:\", output_size)\n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"tanh\")(input_layer)\n",
    "    hidden_layer = layers.Dropout(0.2)(hidden_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(output_size, activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size: 1304 , output_size: 2\n",
      "Epoch 1/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.6792 - acc: 0.5517\n",
      "Epoch 2/20\n",
      "232/232 [==============================] - 0s 190us/step - loss: 0.5323 - acc: 0.9052\n",
      "Epoch 3/20\n",
      "232/232 [==============================] - 0s 205us/step - loss: 0.4156 - acc: 0.9612\n",
      "Epoch 4/20\n",
      "232/232 [==============================] - 0s 204us/step - loss: 0.3167 - acc: 0.9828\n",
      "Epoch 5/20\n",
      "232/232 [==============================] - 0s 188us/step - loss: 0.2370 - acc: 1.0000\n",
      "Epoch 6/20\n",
      "232/232 [==============================] - 0s 207us/step - loss: 0.1761 - acc: 1.0000\n",
      "Epoch 7/20\n",
      "232/232 [==============================] - 0s 188us/step - loss: 0.1313 - acc: 1.0000\n",
      "Epoch 8/20\n",
      "232/232 [==============================] - 0s 180us/step - loss: 0.1015 - acc: 1.0000\n",
      "Epoch 9/20\n",
      "232/232 [==============================] - 0s 181us/step - loss: 0.0758 - acc: 1.0000\n",
      "Epoch 10/20\n",
      "232/232 [==============================] - 0s 184us/step - loss: 0.0622 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "232/232 [==============================] - 0s 226us/step - loss: 0.0500 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "232/232 [==============================] - 0s 221us/step - loss: 0.0416 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "232/232 [==============================] - 0s 193us/step - loss: 0.0352 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "232/232 [==============================] - 0s 198us/step - loss: 0.0295 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "232/232 [==============================] - 0s 210us/step - loss: 0.0261 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "232/232 [==============================] - 0s 205us/step - loss: 0.0231 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "232/232 [==============================] - 0s 193us/step - loss: 0.0199 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "232/232 [==============================] - 0s 195us/step - loss: 0.0187 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "232/232 [==============================] - 0s 192us/step - loss: 0.0164 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "232/232 [==============================] - 0s 197us/step - loss: 0.0146 - acc: 1.0000\n",
      "predictions:\n",
      " [1 1 1]\n",
      "\n",
      "NN, Count Vectors: \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               130500    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 130,702\n",
      "Trainable params: 130,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[1 1 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.89\t0.89\t0.89\tNone\n",
      "Macro\t0.8914\t0.89\t0.8899\tNone\n",
      "input_size: 1304 , output_size: 2\n",
      "Epoch 1/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.6932 - acc: 0.4784\n",
      "Epoch 2/20\n",
      "232/232 [==============================] - 0s 201us/step - loss: 0.6365 - acc: 0.8750\n",
      "Epoch 3/20\n",
      "232/232 [==============================] - 0s 194us/step - loss: 0.5777 - acc: 0.9914\n",
      "Epoch 4/20\n",
      "232/232 [==============================] - 0s 192us/step - loss: 0.5126 - acc: 0.9957\n",
      "Epoch 5/20\n",
      "232/232 [==============================] - 0s 195us/step - loss: 0.4489 - acc: 1.0000\n",
      "Epoch 6/20\n",
      "232/232 [==============================] - 0s 195us/step - loss: 0.3834 - acc: 1.0000\n",
      "Epoch 7/20\n",
      "232/232 [==============================] - 0s 195us/step - loss: 0.3215 - acc: 1.0000\n",
      "Epoch 8/20\n",
      "232/232 [==============================] - 0s 195us/step - loss: 0.2624 - acc: 1.0000\n",
      "Epoch 9/20\n",
      "232/232 [==============================] - 0s 189us/step - loss: 0.2160 - acc: 1.0000\n",
      "Epoch 10/20\n",
      "232/232 [==============================] - 0s 192us/step - loss: 0.1722 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "232/232 [==============================] - 0s 191us/step - loss: 0.1396 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "232/232 [==============================] - 0s 191us/step - loss: 0.1126 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "232/232 [==============================] - 0s 198us/step - loss: 0.0934 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "232/232 [==============================] - 0s 192us/step - loss: 0.0780 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "232/232 [==============================] - 0s 180us/step - loss: 0.0648 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "232/232 [==============================] - 0s 182us/step - loss: 0.0556 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "232/232 [==============================] - 0s 179us/step - loss: 0.0475 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "232/232 [==============================] - 0s 196us/step - loss: 0.0420 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "232/232 [==============================] - 0s 200us/step - loss: 0.0371 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "232/232 [==============================] - 0s 206us/step - loss: 0.0330 - acc: 1.0000\n",
      "predictions:\n",
      " [1 1 1]\n",
      "\n",
      "NN, WordLevel TF-IDF: \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1304)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               130500    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 130,702\n",
      "Trainable params: 130,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[1 1 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.88\t0.88\t0.88\tNone\n",
      "Macro\t0.88\t0.88\t0.88\tNone\n",
      "input_size: 3482 , output_size: 2\n",
      "Epoch 1/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.6940 - acc: 0.4957\n",
      "Epoch 2/20\n",
      "232/232 [==============================] - 0s 279us/step - loss: 0.6277 - acc: 0.9310\n",
      "Epoch 3/20\n",
      "232/232 [==============================] - 0s 275us/step - loss: 0.5592 - acc: 0.9871\n",
      "Epoch 4/20\n",
      "232/232 [==============================] - 0s 284us/step - loss: 0.4852 - acc: 1.0000\n",
      "Epoch 5/20\n",
      "232/232 [==============================] - 0s 270us/step - loss: 0.4090 - acc: 1.0000\n",
      "Epoch 6/20\n",
      "232/232 [==============================] - 0s 290us/step - loss: 0.3364 - acc: 1.0000\n",
      "Epoch 7/20\n",
      "232/232 [==============================] - 0s 273us/step - loss: 0.2716 - acc: 1.0000\n",
      "Epoch 8/20\n",
      "232/232 [==============================] - 0s 287us/step - loss: 0.2161 - acc: 1.0000\n",
      "Epoch 9/20\n",
      "232/232 [==============================] - 0s 291us/step - loss: 0.1681 - acc: 1.0000\n",
      "Epoch 10/20\n",
      "232/232 [==============================] - 0s 291us/step - loss: 0.1340 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "232/232 [==============================] - 0s 278us/step - loss: 0.1089 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "232/232 [==============================] - 0s 287us/step - loss: 0.0874 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "232/232 [==============================] - 0s 277us/step - loss: 0.0730 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "232/232 [==============================] - 0s 270us/step - loss: 0.0614 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "232/232 [==============================] - 0s 275us/step - loss: 0.0522 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "232/232 [==============================] - 0s 282us/step - loss: 0.0454 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "232/232 [==============================] - 0s 287us/step - loss: 0.0406 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "232/232 [==============================] - 0s 277us/step - loss: 0.0357 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "232/232 [==============================] - 0s 278us/step - loss: 0.0326 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "232/232 [==============================] - 0s 287us/step - loss: 0.0289 - acc: 1.0000\n",
      "predictions:\n",
      " [1 1 1]\n",
      "\n",
      "NN, N-Gram Vectors: \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 3482)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               348300    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 348,502\n",
      "Trainable params: 348,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[1 1 1 1 1 0 1 1 1 1]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.74\t0.74\t0.74\tNone\n",
      "Macro\t0.8118\t0.74\t0.7241\tNone\n",
      "input_size: 6583 , output_size: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.6790 - acc: 0.6293\n",
      "Epoch 2/20\n",
      "232/232 [==============================] - 0s 456us/step - loss: 0.5484 - acc: 0.9914\n",
      "Epoch 3/20\n",
      "232/232 [==============================] - 0s 456us/step - loss: 0.4249 - acc: 1.0000\n",
      "Epoch 4/20\n",
      "232/232 [==============================] - 0s 464us/step - loss: 0.3156 - acc: 1.0000\n",
      "Epoch 5/20\n",
      "232/232 [==============================] - 0s 461us/step - loss: 0.2284 - acc: 1.0000\n",
      "Epoch 6/20\n",
      "232/232 [==============================] - 0s 461us/step - loss: 0.1607 - acc: 1.0000\n",
      "Epoch 7/20\n",
      "232/232 [==============================] - 0s 460us/step - loss: 0.1150 - acc: 1.0000\n",
      "Epoch 8/20\n",
      "232/232 [==============================] - 0s 456us/step - loss: 0.0839 - acc: 1.0000\n",
      "Epoch 9/20\n",
      "232/232 [==============================] - 0s 458us/step - loss: 0.0616 - acc: 1.0000\n",
      "Epoch 10/20\n",
      "232/232 [==============================] - 0s 452us/step - loss: 0.0483 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "232/232 [==============================] - 0s 454us/step - loss: 0.0375 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "232/232 [==============================] - 0s 473us/step - loss: 0.0314 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "232/232 [==============================] - 0s 493us/step - loss: 0.0270 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "232/232 [==============================] - 0s 458us/step - loss: 0.0235 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "232/232 [==============================] - 0s 452us/step - loss: 0.0200 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "232/232 [==============================] - 0s 448us/step - loss: 0.0177 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "232/232 [==============================] - 0s 458us/step - loss: 0.0156 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "232/232 [==============================] - 0s 497us/step - loss: 0.0137 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "232/232 [==============================] - 0s 470us/step - loss: 0.0127 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "232/232 [==============================] - 0s 460us/step - loss: 0.0115 - acc: 1.0000\n",
      "predictions:\n",
      " [1 1 1]\n",
      "\n",
      "NN, CharLevel Vectors: \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 6583)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               658400    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 658,602\n",
      "Trainable params: 658,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[1 1 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.88\t0.88\t0.88\tNone\n",
      "Macro\t0.8806\t0.88\t0.8799\tNone\n",
      "\n",
      "It takes 9.41 seconds for Simple NN.\n"
     ]
    }
   ],
   "source": [
    "def Run_SimpleNN():\n",
    "    \n",
    "    time_SimpleNN = time.time()\n",
    "\n",
    "    classifier = create_SimpleNN(xtrain_count.shape[1], Num_Classes)\n",
    "    predictions = train_NN_predict(classifier, xtrain_count, y_Train_OneHot, \n",
    "                          xtest_count, epochs=20)\n",
    "    print(\"\\nNN, Count Vectors: \")\n",
    "    print(classifier.summary())\n",
    "    show_Result(predictions)\n",
    "\n",
    "    classifier = create_SimpleNN(xtrain_tfidf.shape[1], Num_Classes)\n",
    "    predictions = train_NN_predict(classifier, xtrain_tfidf, y_Train_OneHot, \n",
    "                          xtest_tfidf, epochs=20)\n",
    "    print(\"\\nNN, WordLevel TF-IDF: \")\n",
    "    print(classifier.summary())\n",
    "    show_Result(predictions)\n",
    "\n",
    "    classifier = create_SimpleNN(xtrain_tfidf_ngram.shape[1], Num_Classes)\n",
    "    predictions = train_NN_predict(classifier, xtrain_tfidf_ngram, y_Train_OneHot, \n",
    "                          xtest_tfidf_ngram, epochs=20)\n",
    "    print(\"\\nNN, N-Gram Vectors: \")\n",
    "    print(classifier.summary())\n",
    "    show_Result(predictions)\n",
    "\n",
    "    classifier = create_SimpleNN(xtrain_tfidf_ngram_chars.shape[1], Num_Classes)\n",
    "    predictions = train_NN_predict(classifier, xtrain_tfidf_ngram_chars, y_Train_OneHot, \n",
    "                          xtest_tfidf_ngram_chars, epochs=20)\n",
    "    print(\"\\nNN, CharLevel Vectors: \")\n",
    "    print(classifier.summary())\n",
    "    show_Result(predictions)\n",
    "\n",
    "    print(\"\\nIt takes %4.2f seconds for Simple NN.\"%(time.time()-time_SimpleNN))\n",
    "\n",
    "Run_SimpleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Deep Neural Networks\n",
    "Deep Neural Networks use a cascade of multiple layers of nonlinear processing units for feature extraction and transformation. The figure below is from: https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/OH3gI.png.\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/OH3gI.png)\n",
    "They are more complex neural networks in which the hidden layers performs much more complex operations than simple sigmoid or relu activations. Different types of deep learning models can be applied in text classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.1 Convolutional Neural Network\n",
    "In convolutional neural networks, convolutions over the input layer are used to compute the output. This results in local connections, where each region of the input is connected to a neuron in the output. Each layer applies different filters and combines their results. The figure below is from: https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/cnnimage.png.\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/cnnimage.png)\n",
    "\n",
    "Read more about Convolutional Neural Networks at: https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 30, 300)           391500    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 30, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 28, 100)           90100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 486,752\n",
      "Trainable params: 486,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "232/232 [==============================] - 2s 7ms/step - loss: 0.6675 - acc: 0.6293\n",
      "Epoch 2/10\n",
      "232/232 [==============================] - 0s 867us/step - loss: 0.5391 - acc: 0.8103\n",
      "Epoch 3/10\n",
      "232/232 [==============================] - 0s 857us/step - loss: 0.4133 - acc: 0.8836\n",
      "Epoch 4/10\n",
      "232/232 [==============================] - 0s 870us/step - loss: 0.2895 - acc: 0.9353\n",
      "Epoch 5/10\n",
      "232/232 [==============================] - 0s 849us/step - loss: 0.2055 - acc: 0.9440\n",
      "Epoch 6/10\n",
      "232/232 [==============================] - 0s 857us/step - loss: 0.1538 - acc: 0.9526\n",
      "Epoch 7/10\n",
      "232/232 [==============================] - 0s 873us/step - loss: 0.1106 - acc: 0.9828\n",
      "Epoch 8/10\n",
      "232/232 [==============================] - 0s 903us/step - loss: 0.0721 - acc: 0.9871\n",
      "Epoch 9/10\n",
      "232/232 [==============================] - 0s 855us/step - loss: 0.0457 - acc: 0.9957\n",
      "Epoch 10/10\n",
      "232/232 [==============================] - 0s 874us/step - loss: 0.0323 - acc: 1.0000\n",
      "predictions:\n",
      " [1 0 1]\n",
      "CNN, Word Embeddings\n",
      "[1 0 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.87\t0.87\t0.87\tNone\n",
      "Macro\t0.8737\t0.87\t0.8696\tNone\n",
      "\n",
      "It takes 5.10 seconds for Convolutional NN.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "time_CNN = time.time()\n",
    "\n",
    "def create_cnn(input_size, output_size):\n",
    "    # to receive sequences of TextWordsLen integers, between 1 and word_index_len.\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    # see: https://keras.io/getting-started/functional-api-guide/\n",
    "    embedding_layer = layers.Embedding(input_dim=(word_index_len + 1),\n",
    "                output_dim=embedding_vector_size, \n",
    "#                weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "# Making embedding vectors trainable improve microF1 from 0.58 to 0.79 !\n",
    "                weights=[embedding_matrix], trainable=Trainable)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    # output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "    output_layer2 = layers.Dense(output_size, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), \n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def Run_CNN():\n",
    "    classifier = create_cnn(TextWordsLen, Num_Classes)\n",
    "    print(classifier.summary())\n",
    "\n",
    "    predictions = train_NN_predict(classifier, train_seq_x, y_Train_OneHot, \n",
    "#                               test_seq_x, epochs=10) # for CnonC dataset, 10 is better than 20\n",
    "                               test_seq_x, epochs=20)\n",
    "    print(\"CNN, Word Embeddings\")\n",
    "    show_Result(predictions)\n",
    "\n",
    "    print(\"\\nIt takes %4.2f seconds for Convolutional NN.\"%(time.time()-time_CNN))\n",
    "\n",
    "Run_CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2 Recurrent Neural Network – LSTM\n",
    "Unlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a ‘memory state’ of the neurons. This state allows the neurons an ability to remember what have been learned so far.\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/bptt-768x313.png)\n",
    "\n",
    "The memory state in RNNs gives an advantage over traditional neural networks but a problem called Vanishing Gradient is associated with them. In this problem, while learning with a large number of layers, it becomes really hard for the network to learn and tune the parameters of the earlier layers. To address this problem, A new type of RNNs called LSTMs (Long Short Term Memory) Models have been developed.\n",
    "\n",
    "Read more about LSTMs at: https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time_LSTM = time.time()\n",
    "\n",
    "def create_rnn_lstm(input_size, output_size):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(input_dim=(word_index_len + 1),\n",
    "                output_dim=embedding_vector_size, \n",
    "                weights=[embedding_matrix], trainable=Trainable)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    # output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "    output_layer2 = layers.Dense(output_size, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), \n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def Run_LSTM():\n",
    "    classifier = create_rnn_lstm(TextWordsLen, Num_Classes)\n",
    "    print(classifier.summary())\n",
    "\n",
    "    predictions = train_NN_predict(classifier, train_seq_x, y_Train_OneHot, \n",
    "                               test_seq_x, epochs=20)\n",
    "    print(\"RNN-LSTM, Word Embeddings\")\n",
    "    show_Result(predictions)\n",
    "\n",
    "    print(\"\\nIt takes %4.2f seconds for LSTM.\"%(time.time()-time_LSTM))\n",
    "\n",
    "#Run_LSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.3 Recurrent Neural Network – GRU\n",
    "Gated Recurrent Units are another form of recurrent neural networks. Lets add a layer of GRU instead of LSTM in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time_GRU = time.time()\n",
    "\n",
    "def create_rnn_gru(input_size, output_size):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(input_dim=(word_index_len + 1),\n",
    "                output_dim=embedding_vector_size, \n",
    "                weights=[embedding_matrix], trainable=Trainable)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    # output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "    output_layer2 = layers.Dense(output_size, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), \n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def Run_GRU():\n",
    "    classifier = create_rnn_gru(TextWordsLen, Num_Classes)\n",
    "    print(classifier.summary())\n",
    "\n",
    "    predictions = train_NN_predict(classifier, train_seq_x, y_Train_OneHot, \n",
    "                               test_seq_x, epochs=20)\n",
    "    print(\"RNN-GRU, Word Embeddings\")\n",
    "    show_Result(predictions)\n",
    "\n",
    "    print(\"\\nIt takes %4.2f seconds for GRU.\"%(time.time()-time_GRU))\n",
    "\n",
    "#Run_GRU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.4 Bidirectional RNN\n",
    "RNN layers can be wrapped in Bidirectional layers as well. Lets wrap our GRU layer in bidirectional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time_BiGRU = time.time()\n",
    "\n",
    "def create_bidirectional_rnn(input_size, output_size):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(input_dim=(word_index_len + 1),\n",
    "                output_dim=embedding_vector_size, \n",
    "            weights=[embedding_matrix], trainable=Trainable)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    # output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "    output_layer2 = layers.Dense(output_size, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), \n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def Run_BiGRU():\n",
    "    classifier = create_bidirectional_rnn(TextWordsLen, Num_Classes)\n",
    "    print(classifier.summary())\n",
    "\n",
    "    predictions = train_NN_predict(classifier, train_seq_x, y_Train_OneHot, \n",
    "                               test_seq_x, epochs=20)\n",
    "    print(\"Bidirectional-GRU, Word Embeddings\")\n",
    "    show_Result(predictions)\n",
    "\n",
    "    print(\"\\nIt takes %4.2f seconds for Bidirectional GRU.\"%(time.time()-time_BiGRU))\n",
    "\n",
    "#Run_BiGRU() # may require 2 times of clock time than GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.5 Recurrent Convolutional Neural Network (RCNN)\n",
    "\n",
    "The architechture of RCNN in the following is very similar to:\n",
    "![](https://www.researchgate.net/publication/329109642/figure/fig3/AS:695607987535873@1542857276692/The-architecture-of-the-BLSTM-C-model_W640.jpg)\n",
    "which is from Yue Li, Xutao Wang, Pengjian Xu, \"Chinese Text Classification Model Based on Deep Learning\", Future Internet, 10(11), 2018, doi:10.3390/fi10110113www.mdpi.com/journal/futureinternet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 30, 300)           391500    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 30, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 30, 100)           105300    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 28, 100)           30100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 532,052\n",
      "Trainable params: 532,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "232/232 [==============================] - 4s 19ms/step - loss: 0.6843 - acc: 0.5517\n",
      "Epoch 2/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.5712 - acc: 0.8491\n",
      "Epoch 3/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4298 - acc: 0.8664\n",
      "Epoch 4/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2619 - acc: 0.9353\n",
      "Epoch 5/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.1656 - acc: 0.9483\n",
      "Epoch 6/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.1073 - acc: 0.9526\n",
      "Epoch 7/20\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.0386 - acc: 0.9914\n",
      "Epoch 8/20\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.0201 - acc: 0.9914\n",
      "Epoch 9/20\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.0096 - acc: 0.9957\n",
      "Epoch 10/20\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0057 - acc: 0.9957\n",
      "Epoch 16/20\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 8.7274e-04 - acc: 1.0000\n",
      "predictions:\n",
      " [1 0 1]\n",
      "RCNN, Word Embeddings\n",
      "[1 0 1 1 0 0 1 1 1 0]\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.9\t0.9\t0.9\tNone\n",
      "Macro\t0.9025\t0.9\t0.8998\tNone\n",
      "\n",
      "It takes 22.32 seconds for RCNN CNN.\n",
      "\n",
      "It takes 655.69 seconds for all the experiments.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "time_RCNN = time.time()\n",
    "\n",
    "def create_rcnn(input_size, output_size):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(input_dim=(word_index_len + 1),\n",
    "                output_dim=embedding_vector_size, \n",
    "                weights=[embedding_matrix], trainable=Trainable)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer)\n",
    "    \n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    #conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "    # The above line is replaced by the next line on 2019/02/03\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(rnn_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    # output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "    output_layer2 = layers.Dense(output_size, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), \n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def Run_RCNN():\n",
    "    classifier = create_rcnn(TextWordsLen, Num_Classes)\n",
    "    print(classifier.summary())\n",
    "\n",
    "    predictions = train_NN_predict(classifier, train_seq_x, y_Train_OneHot, \n",
    "                               test_seq_x, epochs=20)\n",
    "    print(\"RCNN, Word Embeddings\")\n",
    "    show_Result(predictions)\n",
    "\n",
    "    print(\"\\nIt takes %4.2f seconds for RCNN CNN.\"%(time.time()-time_RCNN))\n",
    "\n",
    "Run_RCNN()\n",
    "\n",
    "print(\"\\nIt takes %4.2f seconds for all the experiments.\"%(time.time()-time_Start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improving Text Classification Models\n",
    "While the above framework can be applied to a number of text classification problems, but to achieve a good accuracy some improvements can be done in the overall framework. For example, following are some tips to improve the performance of text classification models and this framework.\n",
    "\n",
    "1. Text Cleaning : text cleaning can help to reducue the noise present in text data in the form of stopwords, punctuations marks, suffix variations etc. This article can help to understand how to implement text classification in detail.\n",
    "\n",
    "2. Hstacking Text / NLP features with text feature vectors : In the feature engineering section, we generated a number of different feature vectros, combining them together can help to improve the accuracy of the classifier.\n",
    "\n",
    "3. Bidirectional Recurrent Convolutional Neural Networks\n",
    "4. CNNs and RNNs with more number of layers\n",
    "5. Sequence to Sequence Models with Attention\n",
    "6. Hierarichial Attention Networks\n",
    "\n",
    "7. Hyperparamter Tuning in modelling : Tuning the paramters is an important step, a number of parameters such as tree length, leafs, network paramters etc can be fine tuned to get a best fit model.\n",
    "\n",
    "8. Ensemble Models : Stacking different models and blending their outputs can help to further improve the results. Read more about ensemble models here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
